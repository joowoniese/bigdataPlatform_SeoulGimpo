{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCLSeEoVZ78w",
        "outputId": "861576b6-937d-4608-e8ab-dc468ae8887c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting koalas\n",
            "  Downloading koalas-0.32.0-py3-none-any.whl (593 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.2/593.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: pandas>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from koalas) (1.5.3)\n",
            "Requirement already satisfied: pyarrow>=0.10 in /usr/local/lib/python3.10/dist-packages (from koalas) (10.0.1)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.10/dist-packages (from koalas) (1.23.5)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from koalas) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->koalas) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->koalas) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->koalas) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->koalas) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->koalas) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->koalas) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->koalas) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->koalas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.2->koalas) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->koalas) (1.16.0)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=9719a93346e9c814eeaf31e3aae8c5fc1d16ab11d8ada2ba7a88f0e056830a1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark, koalas\n",
            "Successfully installed koalas-0.32.0 pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark koalas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rwz0Ib0jaHtl",
        "outputId": "53995651-764a-4cda-8534-194b400ef635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5J4EG0PsaKmb",
        "outputId": "872191c1-eb22-47ec-a771-e6125a9a1012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, col, split\n",
        "from pyspark.sql.types import StringType, ArrayType\n",
        "from pyspark.sql.functions import udf\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Spark 세션 생성\n",
        "spark = SparkSession.builder.appName(\"SentimentAnalysis\").getOrCreate()\n",
        "\n",
        "# CSV 파일 불러오기\n",
        "file_path = \"/content/inti_op.csv\"\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# 'text' 컬럼을 선택하고 배열로 변환\n",
        "df = df.withColumn(\"text\", split(df[\"text\"], \",\\s*\").cast(ArrayType(StringType())))\n",
        "\n",
        "# 'text' 컬럼을 선택하고 explode를 사용하여 배열 풀기\n",
        "df_text = df.select(explode(col(\"text\")).alias(\"text\"))\n",
        "\n",
        "# TextBlob을 사용한 감정 분석 함수 정의\n",
        "def analyze_sentiment(text):\n",
        "    analysis = TextBlob(text)\n",
        "    # 감정 점수에 따라 긍정/중립/부정으로 분류\n",
        "    if analysis.sentiment.polarity > 0.5:\n",
        "        return \"positive\"\n",
        "    elif analysis.sentiment.polarity == 0.5:\n",
        "        return \"neutral\"\n",
        "    else:\n",
        "        return \"negative\"\n",
        "\n",
        "# PySpark DataFrame의 특정 컬럼에 사용자 정의 함수를 적용하기 위해 udf 생성\n",
        "sentiment_udf = udf(analyze_sentiment, StringType())\n",
        "\n",
        "# 'text' 컬럼에 대해 감정 분석 수행하여 'sentiment' 컬럼 추가\n",
        "df_text = df_text.withColumn(\"sentiment\", sentiment_udf(\"text\"))\n",
        "\n",
        "# 결과 출력\n",
        "df_text.show(truncate=False)\n",
        "\n",
        "# 결과를 CSV 파일로 저장\n",
        "result_path = \"/content/inti_result.csv\"\n",
        "df_text.select(\"text\", \"sentiment\").coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(result_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciuD4SbHYFmv",
        "outputId": "823f9ff0-45ec-484f-d9fd-3b7a9b70eaa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------------------------------+---------+\n",
            "|text                                                                                                 |sentiment|\n",
            "+-----------------------------------------------------------------------------------------------------+---------+\n",
            "|나는 만약 김포에서 서울 산다고 하면 좀 어이없을 듯                                                   |negative |\n",
            "|김포 서울 편입한다더니 이젠 양산김해 부산 편입까지 말나오는게 진짜 어이없다                          |negative |\n",
            "|내년에 지방직 공무원 안뽑나                                                                          |negative |\n",
            "|김포뿐 아니라 과천                                                                                   |negative |\n",
            "|하남                                                                                                 |negative |\n",
            "|고양 등도 서울편입 논의한다던데… 이러면 경기도 다 죽는 거 아닌가… 알짜배기 지자체 다 떼서 서울 주면… |negative |\n",
            "|그냥 경기 분도                                                                                       |negative |\n",
            "|서울 편입 둘 다 안 하는 게 좋은 거 같은데                                                            |negative |\n",
            "|그냥 쓸데없이 행정구역 양쪽 당 모두 건들지 않았으면 좋겠다                                           |negative |\n",
            "|서울 편입은 사실 보여주기식인데                                                                      |negative |\n",
            "|시민들은 안중에도 없는듯                                                                             |negative |\n",
            "|김포나 서울 편입되는 경기도들은 완전 좋겠다                                                          |negative |\n",
            "|국힘이 제정신인지 의문이다                                                                           |negative |\n",
            "|여론조사도 서울이나 경기도나 60%이상이 편입 반대함                                                   |negative |\n",
            "|서울 입장에선 김포 받아주고 그쪽에 매립지 지어야하는 상황                                            |negative |\n",
            "|총선승리 카드로 국힘이 쓰는거                                                                        |negative |\n",
            "|김포나 서울 편입 되는 경기도 지역은 완전 신분상승이네                                                |negative |\n",
            "|김포시장이 서울시되면 쓰레기 소각장 가능하다고 했는데                                                |negative |\n",
            "|쓰레기 소각장 및 혐오시설 개꿀                                                                       |negative |\n",
            "|서울에서는 반대 60.6%                                                                                |negative |\n",
            "+-----------------------------------------------------------------------------------------------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, col, split\n",
        "from pyspark.sql.types import StringType, ArrayType\n",
        "from pyspark.sql.functions import udf\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Spark 세션 생성\n",
        "spark = SparkSession.builder.appName(\"SentimentAnalysis\").getOrCreate()\n",
        "\n",
        "# CSV 파일 불러오기\n",
        "file_path = \"/content/seoulnp_new.csv\"\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# 'text' 컬럼을 선택하고 배열로 변환\n",
        "df = df.withColumn(\"text\", split(df[\"text\"], \",\\s*\").cast(ArrayType(StringType())))\n",
        "\n",
        "# 'text' 컬럼을 선택하고 explode를 사용하여 배열 풀기\n",
        "df_text = df.select(explode(col(\"text\")).alias(\"text\"))\n",
        "\n",
        "# TextBlob을 사용한 감정 분석 함수 정의\n",
        "def analyze_sentiment(text):\n",
        "    analysis = TextBlob(text)\n",
        "    # 감정 점수에 따라 긍정/중립/부정으로 분류\n",
        "    if analysis.sentiment.polarity > -0.1:\n",
        "        return \"positive\"\n",
        "    elif analysis.sentiment.polarity == -0.1:\n",
        "        return \"neutral\"\n",
        "    else:\n",
        "        return \"negative\"\n",
        "\n",
        "# PySpark DataFrame의 특정 컬럼에 사용자 정의 함수를 적용하기 위해 udf 생성\n",
        "sentiment_udf = udf(analyze_sentiment, StringType())\n",
        "\n",
        "# 'text' 컬럼에 대해 감정 분석 수행하여 'sentiment' 컬럼 추가\n",
        "df_text = df_text.withColumn(\"sentiment\", sentiment_udf(\"text\"))\n",
        "\n",
        "# 결과 출력\n",
        "df_text.show(truncate=False)\n",
        "\n",
        "# 결과를 CSV 파일로 저장\n",
        "result_path = \"/content/seoulnp_sentimen1.csv\"\n",
        "df_text.select(\"text\", \"sentiment\").coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(result_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSuZiFChfuK1",
        "outputId": "9793f5b7-8f8b-44a7-f050-68e269ec8ea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------+---------+\n",
            "|text                                                                   |sentiment|\n",
            "+-----------------------------------------------------------------------+---------+\n",
            "|국토장관 후보자                                                        |positive |\n",
            "|‘메가서울’ 논란에 “국토 효율화 위해 최선”                              |positive |\n",
            "|‘김포 서울 편입’논란에 힘 빠진 경기북부특별자치도 재점화               |positive |\n",
            "|김포                                                                   |positive |\n",
            "|서울 편입 잰걸음… 기후동행카드도 동참                                  |positive |\n",
            "|김포시                                                                 |positive |\n",
            "|서울 편입 행정절차 본격화…“주민투표 건의”                              |positive |\n",
            "|[서울광장] 메가서울과 지방균형발전/오일만 세종취재본부장               |positive |\n",
            "|김기현 “野 막가파식 특검·습관성 묻지마 탄핵으로 국회 마비”             |positive |\n",
            "|‘아파트 사잇길’ 심층 분석 인상적… 경제 이슈 종합적으로 다뤄야          |positive |\n",
            "|‘동부경남 소외론’에 박완수 지사 “과거 도정 관심 부족...균형발전 도모”  |positive |\n",
            "|[열린세상] 서울 편입보다 시도 통합이 먼저다/하혜수 경북대 행정학부 교수|positive |\n",
            "|與 “메가시티 비수도권 차별 아냐…‘제로섬’ 아닌 ‘윈윈 게임’”             |positive |\n",
            "|‘메가시티’ 보폭 넓히는 오세훈                                          |positive |\n",
            "|‘북자도’ 홍보 집중 김동연…엇갈린 행보 주목                             |positive |\n",
            "|‘행정통합 논의 다음에’...국힘 뉴시티 특위위원장 경남 방문 취소         |positive |\n",
            "|[열린세상] ‘문화의 분권화’ 시대로 가자/이종수 연세대 행정대학원장      |positive |\n",
            "|野 ‘김포 5호선 연장 예타 면제법안’ 소위 단독 의결                      |positive |\n",
            "|김영미 교수                                                            |positive |\n",
            "|25일 영광에서 출판기념회                                               |positive |\n",
            "+-----------------------------------------------------------------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, col, split\n",
        "from pyspark.sql.types import StringType, ArrayType\n",
        "\n",
        "# Spark 세션 생성\n",
        "spark = SparkSession.builder.appName(\"SentimentAnalysis\").getOrCreate()\n",
        "\n",
        "# CSV 파일 불러오기\n",
        "\n",
        "file_path = \"/content/seoulnp_new.csv\"\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# '0' 컬럼을 선택하고 배열로 변환\n",
        "df = df.withColumn(\"0\", split(df[\"0\"], \",\\s*\").cast(ArrayType(StringType())))\n",
        "\n",
        "# '0' 컬럼을 선택하고 explode를 사용하여 배열 풀기\n",
        "df_text = df.select(explode(col(\"0\")).alias(\"text\"))\n",
        "\n",
        "# TextBlob을 사용한 감정 분석 함수 정의\n",
        "def analyze_sentiment(text):\n",
        "    analysis = TextBlob(text)\n",
        "    # 감정 점수가 0.3보다 크면 긍정, 작으면 부정으로 분류\n",
        "    return \"positive\" if analysis.sentiment.polarity > -0.3 else \"negative\"\n",
        "\n",
        "# PySpark DataFrame의 특정 컬럼에 사용자 정의 함수를 적용하기 위해 udf 생성\n",
        "sentiment_udf = udf(analyze_sentiment, StringType())\n",
        "\n",
        "# 'text' 컬럼에 대해 감정 분석 수행하여 'sentiment' 컬럼 추가\n",
        "df_text = df_text.withColumn(\"sentiment\", sentiment_udf(\"text\"))\n",
        "\n",
        "# 결과 출력\n",
        "df_text.show(truncate=False)\n",
        "\n",
        "# 결과를 CSV 파일로 저장\n",
        "#result_path = \"gimponews_sentiment.csv\"\n",
        "result_path = \"seoulnp_sentiment1.csv\"\n",
        "df_text.select(\"text\", \"sentiment\").coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(result_path)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "7e48WAMkag1_",
        "outputId": "25779ae1-f927-4de4-d685-eb981f4d1a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-40e3c1e74649>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# '0' 컬럼을 선택하고 배열로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\",\\s*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mArrayType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStringType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# '0' 컬럼을 선택하고 explode를 사용하여 배열 풀기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   3072\u001b[0m         \"\"\"\n\u001b[1;32m   3073\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3074\u001b[0;31m             \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3075\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3076\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `0` cannot be resolved. Did you mean one of the following? [`text`]."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#새로운 CSV 파일 생성\n",
        "new_file_content = '''text\n",
        "\"국토장관 후보자, ‘메가서울’ 논란에 “국토 효율화 위해 최선”\"\n",
        "\"‘김포 서울 편입’논란에 힘 빠진 경기북부특별자치도 재점화\"\n",
        "\"김포, 서울 편입 잰걸음… 기후동행카드도 동참\"\n",
        "\"김포시, 서울 편입 행정절차 본격화…“주민투표 건의”\"\n",
        "\"[서울광장] 메가서울과 지방균형발전/오일만 세종취재본부장\"\n",
        "\"김기현 “野 막가파식 특검·습관성 묻지마 탄핵으로 국회 마비”\"\n",
        "\"‘아파트 사잇길’ 심층 분석 인상적… 경제 이슈 종합적으로 다뤄야\"\n",
        "\"‘동부경남 소외론’에 박완수 지사 “과거 도정 관심 부족...균형발전 도모”\"\n",
        "\"[열린세상] 서울 편입보다 시도 통합이 먼저다/하혜수 경북대 행정학부 교수\"\n",
        "\"與 “메가시티 비수도권 차별 아냐…‘제로섬’ 아닌 ‘윈윈 게임’”\"\n",
        "\"‘메가시티’ 보폭 넓히는 오세훈, ‘북자도’ 홍보 집중 김동연…엇갈린 행보 주목\"\n",
        "\"‘행정통합 논의 다음에’...국힘 뉴시티 특위위원장 경남 방문 취소\"\n",
        "\"[열린세상] ‘문화의 분권화’ 시대로 가자/이종수 연세대 행정대학원장\"\n",
        "\"野 ‘김포 5호선 연장 예타 면제법안’ 소위 단독 의결\"\n",
        "\"김영미 교수, 25일 영광에서 출판기념회\"\n",
        "\"국민의힘 ‘1호 영입’ 공들인 ‘113만 유튜버’ 유현준 “거절”\"\n",
        "\"김동연 “북부특별자치도 주민투표 12월중순까지 정부서 답달라”\"\n",
        "\"고양시장 만난 오세훈 “수도권 재편 논의할 정부 협의체를”\"\n",
        "\"與 뉴시티특위, 서울 일괄 편입 ‘행정통합특별법’ 추진\"\n",
        "\"서준오 서울시의원 “시대와 시민에 역행…오세훈 시장 ‘서울공화국’ 중단해야”\"\n",
        "\"부산 상공인·시민단체, 산업은행 부산 이전 “정기국회 내 처리” 촉구\"\n",
        "\"왕정순 서울시의원 “김포시 서울 편입 논란에 서울연구원 정치적 활용 우려”\"\n",
        "\"“구리시, 과천시도…서울 편입될까요?” 민원 ‘봇물’\"\n",
        "\"김포시 “시민 68%가 서울 편입 찬성” 여론조사 결과발표\"\n",
        "\"“김포, 2025년부터 서울 편입… 농어촌 특례는 2030년까지”\"\n",
        "\"‘김포 편입’ 견해차만 확인 후 헤어진 서울·경기·인천 단체장\"\n",
        "\"경기도와 서울시 간 관할구역 변경에 관한 특별법률안 접수 박완수 경남지사 “결혼 우격다짐으로 안 돼...행정통합도 마찬가지”\"\n",
        "\"與, 서울·김포 통합 특별법 오늘 발의…농어촌특례 폐지 유예\"\n",
        "\"與 “서울·김포 통합 특별법 발의”…농어촌특례 유지\"\n",
        "\"[서울 on] 민의로 포장한 정치/명희진 정치부 기자\"\n",
        "\" \"\n",
        "'''\n",
        "\n",
        "# 파일 경로 지정\n",
        "new_file_path = \"seoulnp_new.csv\"\n",
        "\n",
        "# 파일 저장\n",
        "with open(new_file_path, \"w\") as new_file:\n",
        "    new_file.write(new_file_content)"
      ],
      "metadata": {
        "id": "oW5aPH36cs8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# 새로운 CSV 파일 생성\n",
        "new_file_content = '''text\n",
        "\"[서울편입, 무엇이 좋아지나-보건]서울 편입되면 ‘아이 낳고 기르기 좋은 도시’ 된다\"\n",
        "\"김병수 김포시장 “서울 편입, 김포의 정체성은 우리가 만들어 갈 것”\"\n",
        "\"김병수 김포시장 “서울편입, 서울과 김포 윈-윈할 수 있도록 최선”\"\n",
        "\"[서울편입, 무엇이 좋아지나-복지]서울편입되면 ＇보다 살기 좋은 김포＇ 된다\"\n",
        "\"김병수 김포시장 “서울편입, 김포시민들의 삶의 질 달라질 것”\"\n",
        "\"김포시, 시민 소통 박차.. 서울편입 시민 관심 고조\"\n",
        "\"[서울편입, 무엇이 좋아지나 – 교육]서울 편입되면 ‘고교평준화 적용, 특목고·자사고 등 선택 폭 확대’\"\n",
        "\"[서울편입, 무엇이 좋아지나-문화예술]서울편입되면 ＇일상이 문화가 되는 김포＇ 된다\"\n",
        "\"‘소통, 또 소통’ 김병수 김포시장에 단체장들 “서울편입 긍정적”\"\n",
        "\"김병수 김포시장 “서울편입, 총선용 아냐.. 김포특수상황 이해 필요”\"\n",
        "\"김병수 김포시장 소통 박차, 시민들 “서울편입이 타당”\"\n",
        "\"김포시민 68% “김포서울편입 찬성”\"\n",
        "\"김포시, 서울시 편입 편파·오보 종편방송사 언론중재위 제소\"\n",
        "\"[서울편입, 무엇이 좋아지나-보육]서울편입되면 ＇아이 키우기 좋은 김포＇ 된다\"\n",
        "\"김병수 김포시장 “서울편입은 김포시민 삶의 질 향상이 기준”,서울편입·5호선 김포연장 밀도있는 소통 오가\"\n",
        "\"김포 ‘서울편입’, 세수 큰 변동 없다\"\n",
        "\"김포시, 통진읍 마송공영주차장 편입 사유지 소유권이전등기 국가소송 승소\"\n",
        "\"김포시, 포내공원 구 국도 편입 토지 3,137㎡ 소유권 환수\"\n",
        "\"김포시, 대벽저류지 편입 토지 2,921㎡ 소유권 환수\"\n",
        "'''\n",
        "\n",
        "# 파일 경로 지정\n",
        "new_file_path = \"gimponews_new.csv\"\n",
        "\n",
        "# 파일 저장\n",
        "with open(new_file_path, \"w\") as new_file:\n",
        "    new_file.write(new_file_content)\n",
        "'''"
      ],
      "metadata": {
        "id": "xZDiZ9nWaMho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#새로운 파일을 사용하여 DataFrame 생성\n",
        "df_new = spark.read.option(\"header\", \"true\").csv(new_file_path)\n",
        "\n",
        "# 'text' 열에 대해 감정 분석 수행하여 'sentiment' 열 추가\n",
        "df_new = df_new.withColumn(\"sentiment\", sentiment_udf(\"text\"))\n",
        "\n",
        "# 결과 출력\n",
        "df_new.show(truncate=False)\n",
        "\n",
        "# 결과를 CSV 파일로 저장 (덮어쓰기 모드로 설정)\n",
        "result_path_new = \"/content/seoulnp_sentimen1.csv\"\n",
        "df_new.select(\"text\", \"sentiment\").coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(result_path_new)"
      ],
      "metadata": {
        "id": "doFLuggBaR0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f6434c3-5e02-422b-e39c-293c3a5bbe14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------------------+---------+\n",
            "|text                                                                     |sentiment|\n",
            "+-------------------------------------------------------------------------+---------+\n",
            "|국토장관 후보자, ‘메가서울’ 논란에 “국토 효율화 위해 최선”               |positive |\n",
            "|‘김포 서울 편입’논란에 힘 빠진 경기북부특별자치도 재점화                 |positive |\n",
            "|김포, 서울 편입 잰걸음… 기후동행카드도 동참                              |positive |\n",
            "|김포시, 서울 편입 행정절차 본격화…“주민투표 건의”                        |positive |\n",
            "|[서울광장] 메가서울과 지방균형발전/오일만 세종취재본부장                 |positive |\n",
            "|김기현 “野 막가파식 특검·습관성 묻지마 탄핵으로 국회 마비”               |positive |\n",
            "|‘아파트 사잇길’ 심층 분석 인상적… 경제 이슈 종합적으로 다뤄야            |positive |\n",
            "|‘동부경남 소외론’에 박완수 지사 “과거 도정 관심 부족...균형발전 도모”    |positive |\n",
            "|[열린세상] 서울 편입보다 시도 통합이 먼저다/하혜수 경북대 행정학부 교수  |positive |\n",
            "|與 “메가시티 비수도권 차별 아냐…‘제로섬’ 아닌 ‘윈윈 게임’”               |positive |\n",
            "|‘메가시티’ 보폭 넓히는 오세훈, ‘북자도’ 홍보 집중 김동연…엇갈린 행보 주목|positive |\n",
            "|‘행정통합 논의 다음에’...국힘 뉴시티 특위위원장 경남 방문 취소           |positive |\n",
            "|[열린세상] ‘문화의 분권화’ 시대로 가자/이종수 연세대 행정대학원장        |positive |\n",
            "|野 ‘김포 5호선 연장 예타 면제법안’ 소위 단독 의결                        |positive |\n",
            "|김영미 교수, 25일 영광에서 출판기념회                                    |positive |\n",
            "|국민의힘 ‘1호 영입’ 공들인 ‘113만 유튜버’ 유현준 “거절”                  |positive |\n",
            "|김동연 “북부특별자치도 주민투표 12월중순까지 정부서 답달라”              |positive |\n",
            "|고양시장 만난 오세훈 “수도권 재편 논의할 정부 협의체를”                  |positive |\n",
            "|與 뉴시티특위, 서울 일괄 편입 ‘행정통합특별법’ 추진                      |positive |\n",
            "|서준오 서울시의원 “시대와 시민에 역행…오세훈 시장 ‘서울공화국’ 중단해야” |positive |\n",
            "+-------------------------------------------------------------------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 'sentiment' 컬럼만 선택\n",
        "df_sentiment = df_new.select(\"sentiment\")\n",
        "\n",
        "# 결과를 CSV 파일로 저장 (덮어쓰기 모드로 설정)\n",
        "result_path_sentiment = \"gimponews_sentiment1_new.csv\"\n",
        "df_sentiment.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(result_path_sentiment)\n",
        "\n",
        "# CSV 파일 읽기\n",
        "df_result_sentiment = spark.read.csv(result_path_sentiment, header=True)\n",
        "\n",
        "# 데이터프레임 내용 확인\n",
        "df_result_sentiment.show()\n"
      ],
      "metadata": {
        "id": "L3MqRUozacjf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "0be9c677-581a-4644-e599-716e6e69ccc8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-da781b4fdf3b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 'sentiment' 컬럼만 선택\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_sentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 결과를 CSV 파일로 저장 (덮어쓰기 모드로 설정)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresult_path_sentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gimponews_sentiment1_new.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_new' is not defined"
          ]
        }
      ]
    }
  ]
}